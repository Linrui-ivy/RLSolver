RL Methods
==========

This section summarizes the reinforcement-learning–driven algorithms in RLSolver and how they approach combinatorial optimization.

ECO-DQN
--------
ECO-DQN (Evolutionary Combinatorial Optimization Deep Q-Network) integrates the Evolutionary Combinatorial Optimization framework with a Deep Q-Network. It treats each candidate solution as a sequence generated by a policy network and uses Q-learning to update the network parameters. Training follows the pattern in the ElegantRL quickstart guide [1]_, where `train_and_evaluate(args)` trains a DQN agent in the ECO environment:

.. code-block:: python

   from elegantrl.run import train_and_evaluate
   from elegantrl.config import Arguments
   from elegantrl.agents.AgentDQN import AgentDQN
   from rlsolver.envs import ECOEnv

   env = ECOEnv(problem="maxcut")
   args = Arguments(AgentDQN, env)
   train_and_evaluate(args)  # trains a DQN agent on ECOEnv

S2V-DQN
--------
S2V-DQN (Structure2Vec Deep Q-Network) is the approach introduced by Dai et al. in “Learning Combinatorial Optimization Algorithms over Graphs” [2]_. It uses a graph-neural message-passing network (Structure2Vec) to embed each graph into a fixed-length vector capturing both local and global structure. A DQN agent then selects actions (e.g., which edge to cut) based on this embedding.

To train S2V-DQN on a distribution of graphs:

.. code-block:: bash

   python methods/eco_s2v/main.py  # train S2V-DQN on distribution-wise maxcut instances

Once trained, inference can be run by setting `TRAIN_INFERENCE = 1` in `config.py` and re-running the same script.

MCPG
-----
MCPG (Monte Carlo Policy Gradient) estimates the gradient of expected reward by running full Monte Carlo rollouts under the current policy. Each complete-solution sample is scored, and the policy network is updated by back-propagating the reward signal through the sampled trajectories, effectively performing on-policy policy gradient updates.

iSCO
----
iSCO (improved Sampling algorithm for Combinatorial Optimization) instantiates the generic sampling framework with efficiency and parallelism enhancements.  
It leverages discrete‐space Markov Chain Monte Carlo (MCMC) moves together with just‐in‐time compilation for accelerators, achieving faster convergence on high‐quality solutions across diverse combinatorial instances.

Jumanji
--------
Jumanji is a modular RL environment framework tailored for combinatorial problems. It provides standard Gym-style interfaces and helpers, letting you plug in any policy architecture or training algorithm with minimal boilerplate. Jumanji focuses on reproducibility and extensibility, making it easy to benchmark new methods on a wide range of combinatorial tasks.
